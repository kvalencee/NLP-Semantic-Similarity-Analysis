{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0013fd452424f5",
   "metadata": {},
   "source": [
    "# Cabecera\n",
    "\n",
    "**Nombre completo del estudiante**: Valencia Hernandez Kevin Guadalupe\n",
    "**Grupo**: 5BV1\n",
    "**Carrera**: Ingenieria en Inteligencia Artificial\n",
    "**Fecha de última modificación**: 12/05/25\n",
    "\n",
    "---\n",
    "\n",
    "## Descripción detallada del programa\n",
    "\n",
    "Este programa implementa diferentes algoritmos para identificar similitudes entre palabras, frases y documentos, utilizando tanto enfoques semánticos como sintácticos. Se analizan cinco documentos de Project Gutenberg mediante varias técnicas:\n",
    "\n",
    "1. **Similitud de palabras con synsets**: Uso de las métricas de WordNet \"wup_similarity\" y \"path_similarity\" para encontrar términos similares.\n",
    "2. **Similitud de documentos con synsets**: Comparación de frases representativas mediante \"path_similarity\".\n",
    "3. **Similitud de palabras con embedding**: Utilización del modelo pre-entrenado GloVe para identificar términos similares mediante similitud de coseno.\n",
    "4. **Similitud de documentos con embedding**: Aplicación del modelo BERT para encontrar similitud entre documentos.\n",
    "\n",
    "### Datos de entrada:\n",
    "- **Textos de entrada**: Cinco introducciones de libros de Project Gutenberg de temáticas similares.\n",
    "- **Modelos pre-entrenados**: GloVe y BERT para análisis de similitud.\n",
    "\n",
    "---\n",
    "\n",
    "## PRÁCTICA 4: IDENTIFICACIÓN DE PALABRAS, FRASES, Y DOCUMENTOS SIMILARES"
   ]
  },
  {
   "cell_type": "code",
   "id": "76f728246c4f47b2",
   "metadata": {},
   "source": [
    "# Importaciones mínimas necesarias para comenzar\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except:\n",
    "        print(\"El modelo de spaCy no está instalado. Ejecuta: python -m spacy download en_core_web_sm\")\n",
    "except ImportError:\n",
    "    print(\"spaCy no está instalado. Algunas funcionalidades no estarán disponibles.\")\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Importaciones básicas completadas con éxito\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0578761bfe58cbd",
   "metadata": {},
   "source": [
    "## 1. Generación de cuerpo de documentos\n",
    "\n",
    "En esta sección, identificamos y descargamos cinco libros de interés desde Project Gutenberg. Seleccionamos obras de ciencia ficción clásica para mantener una temática similar. De cada libro, extraemos específicamente la introducción para formar nuestro corpus de análisis."
   ]
  },
  {
   "cell_type": "code",
   "id": "ceaefa54-8ecf-4b07-9a0d-2acde0c3b2ad",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "def obtenerIntroduccionLibro1():\n",
    "    url = \"https://www.gutenberg.org/cache/epub/43713/pg43713.txt\"\n",
    "    response = requests.get(url)\n",
    "    texto = response.text\n",
    "    \n",
    "    marcadorInicio = \"INTRODUCTORY.\"\n",
    "    marcadorFin = \"PART I.\"\n",
    "    \n",
    "    indiceInicio = texto.find(marcadorInicio)\n",
    "    if indiceInicio == -1:\n",
    "        return \"No se encontró la introducción\"\n",
    "    \n",
    "    indiceInicio += len(marcadorInicio)\n",
    "    indiceFin = texto.find(marcadorFin, indiceInicio)\n",
    "    \n",
    "    if indiceFin == -1:\n",
    "        return \"No se encontró el fin de la introducción\"\n",
    "    \n",
    "    return texto[indiceInicio:indiceFin].strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91e0c95c42f0caa5",
   "metadata": {},
   "source": [
    "intro1 = obtenerIntroduccionLibro1()\n",
    "print(intro1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2f11397-bb93-4492-9012-ac52457695c7",
   "metadata": {},
   "source": [
    "def obtenerIntroduccionLibro2():\n",
    "    url = \"https://www.gutenberg.org/cache/epub/49211/pg49211.txt\"\n",
    "    response = requests.get(url)\n",
    "    texto = response.text\n",
    "    \n",
    "    marcadorInicio = \"INTRODUCTORY CHAPTER\"\n",
    "    marcadorFin = \"CHAPTER I\"\n",
    "    \n",
    "    indiceInicio = texto.find(marcadorInicio)\n",
    "    if indiceInicio == -1:\n",
    "        return \"No se encontró la introducción\"\n",
    "    \n",
    "    indiceInicio += len(marcadorInicio)\n",
    "    indiceFin = texto.find(marcadorFin, indiceInicio)\n",
    "    \n",
    "    if indiceFin == -1:\n",
    "        return \"No se encontró el fin de la introducción\"\n",
    "    \n",
    "    return texto[indiceInicio:indiceFin].strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4c163d6-3473-461b-a8c2-7b4d8c9d6eb5",
   "metadata": {},
   "source": [
    "intro2 = obtenerIntroduccionLibro2()\n",
    "print(intro2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28aa8876-f668-475b-ba89-ac301a1e6272",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def obtenerIntroduccionLibro3():\n",
    "    url = \"https://www.gutenberg.org/cache/epub/16410/pg16410.txt\"\n",
    "    response = requests.get(url)\n",
    "    texto = response.text\n",
    "    \n",
    "    # Usar regex con grupos de captura para obtener el texto entre INTRODUCTION y CHAPTER II\n",
    "    # El primer grupo captura \"CHAPTER I\\n\\nINTRODUCTION\"\n",
    "    # El segundo grupo captura todo el contenido hasta antes de \"CHAPTER II\"\n",
    "    patron = r\"(CHAPTER I\\s*\\n\\s*\\n\\s*INTRODUCTION\\s*\\n\\s*\\n)([\\s\\S]*?)(?=\\s*CHAPTER II)\"\n",
    "    \n",
    "    coincidencia = re.search(patron, texto)\n",
    "    \n",
    "    if coincidencia:\n",
    "        # Devolver solo el segundo grupo, que contiene el texto de la introducción\n",
    "        return coincidencia.group(2).strip()\n",
    "    else:\n",
    "        return \"No se encontró la introducción\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b2cb604-020d-4c52-a416-7fa66eb8a119",
   "metadata": {},
   "source": [
    "intro3 = obtenerIntroduccionLibro3()\n",
    "print(intro3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55683d9a-e9c6-4ff9-9a55-ed81aab895ec",
   "metadata": {},
   "source": [
    "def obtenerIntroduccionLibro4():\n",
    "    url = \"https://www.gutenberg.org/cache/epub/48010/pg48010.txt\"\n",
    "    response = requests.get(url)\n",
    "    texto = response.text\n",
    "    \n",
    "    # Buscar la cadena \"INTRODUCTION\" seguida por cualquier cosa hasta \"CHAPTER I\"\n",
    "    primera_intro = texto.find(\"INTRODUCTION\")\n",
    "    if primera_intro == -1:\n",
    "        return \"No se encontró INTRODUCTION\"\n",
    "    \n",
    "    # Buscar la segunda aparición de \"INTRODUCTION\"\n",
    "    segunda_intro = texto.find(\"INTRODUCTION\", primera_intro + 1)\n",
    "    if segunda_intro == -1:\n",
    "        return \"No se encontró la segunda ocurrencia de INTRODUCTION\"\n",
    "    \n",
    "    # Avanzar después de la palabra \"INTRODUCTION\"\n",
    "    inicio_contenido = segunda_intro + len(\"INTRODUCTION\")\n",
    "    \n",
    "    # Buscar CHAPTER I \n",
    "    fin_pos = texto.find(\"CHAPTER I\", inicio_contenido)\n",
    "    if fin_pos == -1:\n",
    "        return \"No se encontró CHAPTER I después de la segunda INTRODUCTION\"\n",
    "    \n",
    "    # Extraer el texto entre inicio_contenido y fin_pos\n",
    "    introduccion_texto = texto[inicio_contenido:fin_pos].strip()\n",
    "    \n",
    "    return introduccion_texto"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45498e77-e8dc-4c79-8b51-ec588792396f",
   "metadata": {},
   "source": [
    "intro4 = obtenerIntroduccionLibro4()\n",
    "print(intro4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad217f15-a243-4ee2-8ebb-c9d4d707a564",
   "metadata": {},
   "source": [
    "def obtenerIntroduccionLibro5():\n",
    "    url = \"https://www.gutenberg.org/cache/epub/26163/pg26163.txt\"\n",
    "    response = requests.get(url)\n",
    "    texto = response.text\n",
    "    \n",
    "    # Buscar la primera ocurrencia de \"INTRODUCTION\"\n",
    "    primera_intro = texto.find(\"INTRODUCTION\")\n",
    "    if primera_intro == -1:\n",
    "        return \"No se encontró INTRODUCTION\"\n",
    "    \n",
    "    # Buscar la segunda aparición de \"INTRODUCTION\"\n",
    "    segunda_intro = texto.find(\"INTRODUCTION\", primera_intro + 1)\n",
    "    if segunda_intro == -1:\n",
    "        return \"No se encontró la segunda ocurrencia de INTRODUCTION\"\n",
    "    \n",
    "    # Avanzar después de la palabra \"INTRODUCTION\"\n",
    "    inicio_contenido = segunda_intro + len(\"INTRODUCTION\")\n",
    "    \n",
    "    # Buscar CHAPTER I\n",
    "    fin_pos = texto.find(\"CHAPTER I\", inicio_contenido)\n",
    "    if fin_pos == -1:\n",
    "        return \"No se encontró CHAPTER I después de la segunda INTRODUCTION\"\n",
    "    \n",
    "    # Extraer el texto entre inicio_contenido y fin_pos\n",
    "    introduccion_texto = texto[inicio_contenido:fin_pos].strip()\n",
    "    \n",
    "    return introduccion_texto"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b82cdeb6-656b-4a1d-a47b-1b6e368fe18f",
   "metadata": {},
   "source": [
    "intro5 = obtenerIntroduccionLibro5()\n",
    "print(intro5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "784aa9f3-557d-497b-bd0f-1f2d2f7b75db",
   "metadata": {},
   "source": [
    "cuerpo = [intro1, intro2, intro3, intro4, intro5]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f7e56ea-d25f-4304-af84-e300187c7254",
   "metadata": {},
   "source": [
    "## 2. Normalización de documentos\n",
    "\n",
    "En esta sección, realizamos la normalización y el preprocesamiento de cada documento del corpus. El proceso incluye:\n",
    "\n",
    "1. Segmentación en oraciones\n",
    "2. Tokenización de palabras\n",
    "3. Etiquetado gramatical (POS tagging)\n",
    "4. Aplicación de técnicas específicas de normalización\n",
    "\n",
    "Estas técnicas de normalización se adaptan según los objetivos de cada análisis:\n",
    "- Para similitud de palabras con synsets, necesitamos mantener la estructura de las oraciones y categorizar por POS\n",
    "- Para similitud con embeddings, necesitamos eliminar stopwords y realizar lematización"
   ]
  },
  {
   "cell_type": "code",
   "id": "8806ec58-13ea-417c-b84d-b691769538f3",
   "metadata": {},
   "source": [
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "\n",
    "# Cargar el modelo de spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def normalizarDocumento(texto, tipo_normalizacion=\"general\"):\n",
    "    \"\"\"\n",
    "    Normaliza un documento segmentándolo en oraciones y tokenizándolo con etiquetas gramaticales,\n",
    "    usando spaCy para el procesamiento.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto a normalizar\n",
    "        tipo_normalizacion (str): Tipo de normalización a aplicar ('general', 'synsets', 'embeddings')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con el texto normalizado según diferentes niveles\n",
    "    \"\"\"\n",
    "    resultado = {\n",
    "        \"oraciones_originales\": [],\n",
    "        \"oraciones_tokenizadas\": [],\n",
    "        \"oraciones_etiquetadas\": [],\n",
    "        \"oraciones_normalizadas\": []\n",
    "    }\n",
    "    \n",
    "    # Limpieza previa del texto completo\n",
    "    # Reemplazar saltos de línea y otros caracteres especiales con espacios\n",
    "    texto = re.sub(r'[\\r\\n\\t]+', ' ', texto)\n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    \n",
    "    # 1. Segmentación en oraciones (usando NLTK para mantener compatibilidad)\n",
    "    oraciones = sent_tokenize(texto)\n",
    "    resultado[\"oraciones_originales\"] = oraciones\n",
    "    \n",
    "    for oracion in oraciones:\n",
    "        # Limpieza adicional de la oración\n",
    "        oracion = oracion.strip()\n",
    "        \n",
    "        # Procesar con spaCy\n",
    "        doc = nlp(oracion)\n",
    "        \n",
    "        # 2. Tokenización (sin puntuación ni números)\n",
    "        tokens = [token.text for token in doc \n",
    "                 if not token.is_punct and not token.like_num]\n",
    "        resultado[\"oraciones_tokenizadas\"].append(tokens)\n",
    "        \n",
    "        # 3. Etiquetado gramatical\n",
    "        tokens_etiquetados = [(token.text, token.pos_) for token in doc \n",
    "                             if not token.is_punct and not token.like_num]\n",
    "        resultado[\"oraciones_etiquetadas\"].append(tokens_etiquetados)\n",
    "        \n",
    "        # 4. Normalización específica según el tipo\n",
    "        if tipo_normalizacion == \"synsets\":\n",
    "            # Para synsets: eliminar stopwords pero mantener la forma original\n",
    "            tokens_normalizados = [token.text for token in doc \n",
    "                                  if not token.is_punct and not token.like_num \n",
    "                                  and not token.is_stop]\n",
    "            \n",
    "        elif tipo_normalizacion == \"embeddings\":\n",
    "            # Para embeddings: eliminar stopwords y lematizar\n",
    "            tokens_normalizados = [token.lemma_ for token in doc \n",
    "                                  if not token.is_punct and not token.like_num \n",
    "                                  and not token.is_stop]\n",
    "        else:  # general\n",
    "            # Normalización general: eliminar stopwords\n",
    "            tokens_normalizados = [token.text for token in doc \n",
    "                                  if not token.is_punct and not token.like_num \n",
    "                                  and not token.is_stop]\n",
    "        \n",
    "        resultado[\"oraciones_normalizadas\"].append(tokens_normalizados)\n",
    "    \n",
    "    return resultado"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e838a12-fedb-49e6-b916-0868db5a9d3e",
   "metadata": {},
   "source": [
    "# Normalización de la introducción 1\n",
    "print(\"Normalizando introducción 1...\")\n",
    "intro1_normalizada = normalizarDocumento(intro1)\n",
    "\n",
    "# Mostrar estadísticas\n",
    "print(f\"Oraciones originales: {len(intro1_normalizada['oraciones_originales'])}\")\n",
    "print(f\"Tokens totales: {sum(len(tokens) for tokens in intro1_normalizada['oraciones_tokenizadas'])}\")\n",
    "print(f\"Tokens después de normalización: {sum(len(tokens) for tokens in intro1_normalizada['oraciones_normalizadas'])}\")\n",
    "\n",
    "# Mostrar ejemplo de normalización completa para la primera oración\n",
    "print(\"\\nEjemplo completo con la primera oración:\")\n",
    "if intro1_normalizada['oraciones_originales']:\n",
    "    primera_oracion = intro1_normalizada['oraciones_originales'][0]\n",
    "    print(f\"\\nOriginal: {primera_oracion}\")\n",
    "    \n",
    "    print(\"\\nTokenizada:\")\n",
    "    print(intro1_normalizada['oraciones_tokenizadas'][0])\n",
    "    \n",
    "    print(\"\\nEtiquetada gramaticalmente:\")\n",
    "    print(intro1_normalizada['oraciones_etiquetadas'][0])\n",
    "    \n",
    "    print(\"\\nNormalizada (sin stopwords):\")\n",
    "    print(intro1_normalizada['oraciones_normalizadas'][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe4150f1-8a05-4baf-ad38-a12654a3b4bf",
   "metadata": {},
   "source": [
    "# Normalización de la introducción 1\n",
    "print(\"Normalizando introducción 1...\")\n",
    "intro1_normalizada = normalizarDocumento(intro1)\n",
    "\n",
    "# Mostrar estadísticas\n",
    "print(f\"Oraciones originales: {len(intro1_normalizada['oraciones_originales'])}\")\n",
    "print(f\"Tokens totales: {sum(len(tokens) for tokens in intro1_normalizada['oraciones_tokenizadas'])}\")\n",
    "print(f\"Tokens después de normalización: {sum(len(tokens) for tokens in intro1_normalizada['oraciones_normalizadas'])}\")\n",
    "\n",
    "# Mostrar ejemplo de normalización completa para la primera oración\n",
    "print(\"\\nEjemplo completo con la primera oración:\")\n",
    "if intro1_normalizada['oraciones_originales']:\n",
    "    primera_oracion = intro1_normalizada['oraciones_originales'][0]\n",
    "    print(f\"\\nOriginal: {primera_oracion}\")\n",
    "    \n",
    "    print(\"\\nTokenizada:\")\n",
    "    print(intro1_normalizada['oraciones_tokenizadas'][0])\n",
    "    \n",
    "    print(\"\\nEtiquetada gramaticalmente:\")\n",
    "    print(intro1_normalizada['oraciones_etiquetadas'][0])\n",
    "    \n",
    "    print(\"\\nNormalizada (sin stopwords):\")\n",
    "    print(intro1_normalizada['oraciones_normalizadas'][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cuerpo = [intro1, intro2, intro3, intro4, intro5]",
   "id": "6b7bd1753cbc5a7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4d43804-b888-411d-bb9d-73ff8827ae91",
   "metadata": {},
   "source": [
    "# Normalizar los 5 documentos para synsets y embeddings\n",
    "documentos_synsets = []\n",
    "documentos_embeddings = []\n",
    "\n",
    "for i, doc in enumerate(cuerpo):\n",
    "    print(f\"Normalizando documento {i+1}...\")\n",
    "    \n",
    "    # Normalización para synsets\n",
    "    doc_synsets = normalizarDocumento(doc, tipo_normalizacion=\"synsets\")\n",
    "    documentos_synsets.append(doc_synsets)\n",
    "    \n",
    "    # Normalización para embeddings\n",
    "    doc_embeddings = normalizarDocumento(doc, tipo_normalizacion=\"embeddings\")\n",
    "    documentos_embeddings.append(doc_embeddings)\n",
    "    \n",
    "    # Mostrar estadísticas básicas\n",
    "    num_oraciones = len(doc_synsets[\"oraciones_originales\"])\n",
    "    num_tokens = sum(len(tokens) for tokens in doc_synsets[\"oraciones_tokenizadas\"])\n",
    "    num_tokens_normalizados = sum(len(tokens) for tokens in doc_synsets[\"oraciones_normalizadas\"])\n",
    "    \n",
    "    print(f\"  - Documento {i+1}: {num_oraciones} oraciones, {num_tokens} tokens\")\n",
    "    print(f\"  - Tokens después de normalización para synsets: {num_tokens_normalizados}\")\n",
    "    print(f\"  - Tokens después de normalización para embeddings: {sum(len(tokens) for tokens in doc_embeddings['oraciones_normalizadas'])}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "53dffc0c-5f84-4699-a069-079b85a66743",
   "metadata": {},
   "source": [
    "## 3. Similitud de palabras con synsets\n",
    "\n",
    "En esta sección, utilizaremos WordNet para encontrar palabras similares a los verbos y sustantivos más comunes de cada documento. Se emplearán dos métricas de similitud diferentes:\n",
    "\n",
    "1. **Wu-Palmer Similarity (wup_similarity)**: Mide la similitud basada en la profundidad relativa de dos synsets en la taxonomía de WordNet.\n",
    "2. **Path Similarity (path_similarity)**: Mide la similitud basada en la distancia del camino más corto entre dos synsets.\n",
    "\n",
    "Para cada documento, identificaremos:\n",
    "- El verbo más común\n",
    "- El sustantivo más común\n",
    "- Los 5 términos más similares a cada uno según ambas métricas"
   ]
  },
  {
   "cell_type": "code",
   "id": "34bc3876-5ea6-401f-8c80-061c33f665bb",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Descargar el recurso de WordNet si es necesario\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def encontrarPalabrasFrecuentes(documento_normalizado):\n",
    "    \"\"\"\n",
    "    Encuentra los verbos y sustantivos más comunes en un documento.\n",
    "    \n",
    "    Args:\n",
    "        documento_normalizado (dict): Documento normalizado con etiquetas POS\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (verbo_más_común, sustantivo_más_común)\n",
    "    \"\"\"\n",
    "    # Contador para verbos y sustantivos\n",
    "    verbos = Counter()\n",
    "    sustantivos = Counter()\n",
    "    \n",
    "    # Analizar cada oración etiquetada\n",
    "    for oracion in documento_normalizado[\"oraciones_etiquetadas\"]:\n",
    "        for palabra, pos in oracion:\n",
    "            # En spaCy, 'VERB' para verbos y 'NOUN' para sustantivos\n",
    "            if pos == 'VERB':\n",
    "                verbos[palabra.lower()] += 1\n",
    "            elif pos == 'NOUN':\n",
    "                sustantivos[palabra.lower()] += 1\n",
    "    \n",
    "    # Encontrar los más comunes\n",
    "    verbo_comun = verbos.most_common(1)\n",
    "    sustantivo_comun = sustantivos.most_common(1)\n",
    "    \n",
    "    # Devolver los más comunes, o None si no hay\n",
    "    verbo_mas_comun = verbo_comun[0][0] if verbo_comun else None\n",
    "    sustantivo_mas_comun = sustantivo_comun[0][0] if sustantivo_comun else None\n",
    "    \n",
    "    return verbo_mas_comun, sustantivo_mas_comun\n",
    "\n",
    "def obtenerSimilaresWordNet(palabra, pos, metrica='wup', top_n=5):\n",
    "    \"\"\"\n",
    "    Obtiene las palabras más similares a una dada usando WordNet.\n",
    "    \n",
    "    Args:\n",
    "        palabra (str): Palabra de referencia\n",
    "        pos (str): Categoría gramatical ('n' para sustantivo, 'v' para verbo)\n",
    "        metrica (str): Métrica de similitud ('wup' o 'path')\n",
    "        top_n (int): Número de palabras similares a devolver\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tuplas (palabra, puntuación)\n",
    "    \"\"\"\n",
    "    # Convertir pos de spaCy a formato WordNet\n",
    "    if pos == 'VERB':\n",
    "        pos_wn = 'v'\n",
    "    elif pos == 'NOUN':\n",
    "        pos_wn = 'n'\n",
    "    else:\n",
    "        pos_wn = 'n'  # Por defecto, sustantivo\n",
    "    \n",
    "    # Obtener synsets de la palabra\n",
    "    synsets = wn.synsets(palabra, pos=pos_wn)\n",
    "    if not synsets:\n",
    "        return []\n",
    "    \n",
    "    # Usar el primer synset (más común)\n",
    "    synset = synsets[0]\n",
    "    \n",
    "    # Obtener todos los synsets de la misma categoría\n",
    "    todos_synsets = list(wn.all_synsets(pos=pos_wn))\n",
    "    \n",
    "    # Calcular similitud con cada synset\n",
    "    similitudes = []\n",
    "    for otro_synset in todos_synsets:\n",
    "        # Evitar comparar con el mismo synset\n",
    "        if otro_synset == synset:\n",
    "            continue\n",
    "        \n",
    "        # Calcular similitud según la métrica especificada\n",
    "        if metrica == 'wup':\n",
    "            sim = synset.wup_similarity(otro_synset)\n",
    "        else:  # path\n",
    "            sim = synset.path_similarity(otro_synset)\n",
    "        \n",
    "        if sim:\n",
    "            # Obtener el nombre de la palabra del synset (lema)\n",
    "            nombre = otro_synset.lemma_names()[0]\n",
    "            similitudes.append((nombre, sim))\n",
    "    \n",
    "\n",
    "    similitudes.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similitudes[:top_n]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41889679-2a8d-4358-aa8b-76455880e268",
   "metadata": {},
   "source": [
    "# Encontrar palabras frecuentes en el documento 1\n",
    "verbo_comun, sustantivo_comun = encontrarPalabrasFrecuentes(documentos_synsets[0])\n",
    "\n",
    "print(f\"Documento 1:\")\n",
    "print(f\"Verbo más común: {verbo_comun}\")\n",
    "print(f\"Sustantivo más común: {sustantivo_comun}\")\n",
    "\n",
    "# Encontrar palabras similares usando Wu-Palmer Similarity\n",
    "print(\"\\nPalabras similares al verbo más común usando Wu-Palmer Similarity:\")\n",
    "similares_wup = obtenerSimilaresWordNet(verbo_comun, 'VERB', 'wup')\n",
    "for palabra, puntuacion in similares_wup:\n",
    "    print(f\"  - {palabra}: {puntuacion:.4f}\")\n",
    "\n",
    "print(\"\\nPalabras similares al sustantivo más común usando Wu-Palmer Similarity:\")\n",
    "similares_wup = obtenerSimilaresWordNet(sustantivo_comun, 'NOUN', 'wup')\n",
    "for palabra, puntuacion in similares_wup:\n",
    "    print(f\"  - {palabra}: {puntuacion:.4f}\")\n",
    "\n",
    "# Encontrar palabras similares usando Path Similarity\n",
    "print(\"\\nPalabras similares al verbo más común usando Path Similarity:\")\n",
    "similares_path = obtenerSimilaresWordNet(verbo_comun, 'VERB', 'path')\n",
    "for palabra, puntuacion in similares_path:\n",
    "    print(f\"  - {palabra}: {puntuacion:.4f}\")\n",
    "\n",
    "print(\"\\nPalabras similares al sustantivo más común usando Path Similarity:\")\n",
    "similares_path = obtenerSimilaresWordNet(sustantivo_comun, 'NOUN', 'path')\n",
    "for palabra, puntuacion in similares_path:\n",
    "    print(f\"  - {palabra}: {puntuacion:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1639d37-a07b-48e0-bc28-82c119303a68",
   "metadata": {},
   "source": [
    "# Análisis de similitud para todos los documentos\n",
    "resultados_similitud = []\n",
    "\n",
    "for i, doc_normalizado in enumerate(documentos_synsets):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Documento {i+1}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Encontrar palabras frecuentes\n",
    "    verbo_comun, sustantivo_comun = encontrarPalabrasFrecuentes(doc_normalizado)\n",
    "    \n",
    "    print(f\"Verbo más común: {verbo_comun}\")\n",
    "    print(f\"Sustantivo más común: {sustantivo_comun}\")\n",
    "    \n",
    "    # Resultados para este documento\n",
    "    resultado_doc = {\n",
    "        'verbo': verbo_comun,\n",
    "        'sustantivo': sustantivo_comun,\n",
    "        'verbo_wup': [],\n",
    "        'verbo_path': [],\n",
    "        'sustantivo_wup': [],\n",
    "        'sustantivo_path': []\n",
    "    }\n",
    "    \n",
    "    # Similitud para el verbo\n",
    "    if verbo_comun:\n",
    "        # Wu-Palmer Similarity\n",
    "        print(\"\\nPalabras similares al verbo usando Wu-Palmer Similarity:\")\n",
    "        similares_wup = obtenerSimilaresWordNet(verbo_comun, 'VERB', 'wup')\n",
    "        for palabra, puntuacion in similares_wup:\n",
    "            print(f\"  - {palabra}: {puntuacion:.4f}\")\n",
    "            resultado_doc['verbo_wup'].append((palabra, puntuacion))\n",
    "        \n",
    "        # Path Similarity\n",
    "        print(\"\\nPalabras similares al verbo usando Path Similarity:\")\n",
    "        similares_path = obtenerSimilaresWordNet(verbo_comun, 'VERB', 'path')\n",
    "        for palabra, puntuacion in similares_path:\n",
    "            print(f\"  - {palabra}: {puntuacion:.4f}\")\n",
    "            resultado_doc['verbo_path'].append((palabra, puntuacion))\n",
    "    \n",
    "    # Similitud para el sustantivo\n",
    "    if sustantivo_comun:\n",
    "        # Wu-Palmer Similarity\n",
    "        print(\"\\nPalabras similares al sustantivo usando Wu-Palmer Similarity:\")\n",
    "        similares_wup = obtenerSimilaresWordNet(sustantivo_comun, 'NOUN', 'wup')\n",
    "        for palabra, puntuacion in similares_wup:\n",
    "            print(f\"  - {palabra}: {puntuacion:.4f}\")\n",
    "            resultado_doc['sustantivo_wup'].append((palabra, puntuacion))\n",
    "        \n",
    "        # Path Similarity\n",
    "        print(\"\\nPalabras similares al sustantivo usando Path Similarity:\")\n",
    "        similares_path = obtenerSimilaresWordNet(sustantivo_comun, 'NOUN', 'path')\n",
    "        for palabra, puntuacion in similares_path:\n",
    "            print(f\"  - {palabra}: {puntuacion:.4f}\")\n",
    "            resultado_doc['sustantivo_path'].append((palabra, puntuacion))\n",
    "    \n",
    "    resultados_similitud.append(resultado_doc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19eb83f1-a30e-4e36-ade1-d75145376739",
   "metadata": {},
   "source": [
    "## 4. Similitud de documentos con synsets\n",
    "\n",
    "En esta sección, extraeremos la frase más representativa de cada documento usando el algoritmo RAKE (Rapid Automatic Keyword Extraction), que según los resultados de la práctica 3 demostró ser uno de los más efectivos para la extracción de frases clave.\n",
    "\n",
    "Luego, seleccionaremos el primer documento como base y compararemos su frase representativa con las de los otros cuatro documentos utilizando la métrica Path_similarity de WordNet."
   ]
  },
  {
   "cell_type": "code",
   "id": "5f758bda-9c78-49ea-81a5-a63d25ccf091",
   "metadata": {},
   "source": [
    "# Importamos las bibliotecas necesarias para la extracción de frases clave usando RAKE\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# Verificamos que se hayan descargado los recursos necesarios\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def extraer_frase_representativa(texto, num_frases=1):\n",
    "    \"\"\"\n",
    "    Extrae las frases más representativas de un texto usando RAKE.\n",
    "\n",
    "    Args:\n",
    "        texto (str): Texto de entrada\n",
    "        num_frases (int): Número de frases a extraer\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de frases ordenadas por relevancia\n",
    "    \"\"\"\n",
    "    # Inicializar RAKE\n",
    "    rake = Rake(\n",
    "        stopwords=nltk.corpus.stopwords.words('english'),\n",
    "        include_repeated_phrases=False\n",
    "    )\n",
    "\n",
    "    # Extraer frases clave\n",
    "    rake.extract_keywords_from_text(texto)\n",
    "    frases_clave = rake.get_ranked_phrases()\n",
    "\n",
    "    # Devolver las frases más relevantes (ordenadas por puntuación)\n",
    "    return frases_clave[:num_frases] if frases_clave else []\n",
    "\n",
    "# Extraer la frase más representativa de cada documento\n",
    "frases_representativas = []\n",
    "for i, doc in enumerate(cuerpo):\n",
    "    print(f\"Documento {i+1}:\")\n",
    "    frases = extraer_frase_representativa(doc)\n",
    "    frase = frases[0] if frases else \"No se encontró frase representativa\"\n",
    "    frases_representativas.append(frase)\n",
    "    print(f\"Frase representativa: {frase}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def comparar_similitud_documento(doc_base_idx, frases_representativas):\n",
    "    \"\"\"\n",
    "    Compara la similitud entre la frase representativa de un documento base\n",
    "    y las frases representativas de otros documentos usando Path Similarity.\n",
    "\n",
    "    Args:\n",
    "        doc_base_idx (int): Índice del documento base\n",
    "        frases_representativas (list): Lista de frases representativas de cada documento\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tuplas (índice_documento, puntuación_similitud)\n",
    "    \"\"\"\n",
    "    resultado = []\n",
    "    frase_base = frases_representativas[doc_base_idx]\n",
    "\n",
    "    # Tokenizar la frase base\n",
    "    tokens_base = word_tokenize(frase_base.lower())\n",
    "\n",
    "    # Filtrar solo sustantivos y verbos (más relevantes para comparación semántica)\n",
    "    pos_tags_base = pos_tag(tokens_base)\n",
    "    palabras_base = [palabra for palabra, tag in pos_tags_base\n",
    "                    if tag.startswith('NN') or tag.startswith('VB')]\n",
    "\n",
    "    # Obtener synsets para palabras base\n",
    "    synsets_base = []\n",
    "    for palabra in palabras_base:\n",
    "        synsets = wn.synsets(palabra)\n",
    "        if synsets:\n",
    "            synsets_base.append(synsets[0])\n",
    "\n",
    "    print(f\"\\nComparando documento {doc_base_idx+1} con los demás:\")\n",
    "\n",
    "    for i, frase in enumerate(frases_representativas):\n",
    "        if i == doc_base_idx:\n",
    "            continue\n",
    "\n",
    "        # Tokenizar la frase a comparar\n",
    "        tokens = word_tokenize(frase.lower())\n",
    "\n",
    "        # Filtrar solo sustantivos y verbos\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        palabras = [palabra for palabra, tag in pos_tags\n",
    "                   if tag.startswith('NN') or tag.startswith('VB')]\n",
    "\n",
    "        # Obtener synsets\n",
    "        synsets_doc = []\n",
    "        for palabra in palabras:\n",
    "            synsets = wn.synsets(palabra)\n",
    "            if synsets:\n",
    "                synsets_doc.append(synsets[0])\n",
    "\n",
    "        # Calcular similitud (promedio de similitudes máximas entre synsets)\n",
    "        if synsets_base and synsets_doc:\n",
    "            similitudes = []\n",
    "            for s1 in synsets_base:\n",
    "                max_sim = 0\n",
    "                for s2 in synsets_doc:\n",
    "                    sim = s1.path_similarity(s2)\n",
    "                    if sim and sim > max_sim:\n",
    "                        max_sim = sim\n",
    "                if max_sim > 0:\n",
    "                    similitudes.append(max_sim)\n",
    "\n",
    "            # Promedio de similitudes\n",
    "            similitud_promedio = sum(similitudes) / len(similitudes) if similitudes else 0\n",
    "            resultado.append((i, similitud_promedio))\n",
    "            print(f\"Documento {i+1}: Similitud {similitud_promedio:.4f}\")\n",
    "        else:\n",
    "            resultado.append((i, 0))\n",
    "            print(f\"Documento {i+1}: No se pudo calcular similitud (sin synsets)\")\n",
    "\n",
    "    return resultado"
   ],
   "id": "5b16c02c2d3584a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a707af52-2f13-4668-88ff-6fa2604fa0ac",
   "metadata": {},
   "source": [
    "# Comparar documento 1 con los demás\n",
    "similitudes_doc1 = comparar_similitud_documento(0, frases_representativas)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Frases representativas extraídas:\")\n",
    "for i, frase in enumerate(frases_representativas):\n",
    "    print(f\"Documento {i+1}: {frase}\")"
   ],
   "id": "3ed952413d828fd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b86697f8-a4a3-4b14-a723-2035893a2dac",
   "metadata": {},
   "source": [
    "## 5. Similitud de palabras con \"embedding\"\n",
    "\n",
    "En esta sección, utilizaremos el modelo pre-entrenado GloVe (Global Vectors for Word Representation) \"Wikipedia 2014 + Gigaword 5\" para identificar en cada documento los 5 términos más similares al verbo más frecuente.\n",
    "\n",
    "Para calcular la similitud entre términos, usaremos la medida de similitud de coseno, que es particularmente efectiva para embeddings de palabras."
   ]
  },
  {
   "cell_type": "code",
   "id": "9872646e-14b6-4471-abc3-dc850b117716",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cargar_glove(dim=100):\n",
    "    \"\"\"\n",
    "    Descarga y carga vectores GloVe \"Wikipedia 2014 + Gigaword 5\".\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Dimensión de los vectores (50, 100, 200 o 300)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con palabras como claves y vectores como valores\n",
    "    \"\"\"\n",
    "    # URL del modelo GloVe\n",
    "    url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    \n",
    "    # Nombre del archivo\n",
    "    zip_filename = \"glove.6B.zip\"\n",
    "    txt_filename = f\"glove.6B.{dim}d.txt\"\n",
    "    \n",
    "    # Verificar si ya existe el archivo extraído\n",
    "    if not os.path.exists(txt_filename):\n",
    "        # Verificar si existe el zip\n",
    "        if not os.path.exists(zip_filename):\n",
    "            print(f\"Descargando {zip_filename} desde {url}...\")\n",
    "            print(\"Este proceso puede tardar varios minutos dependiendo de tu conexión.\")\n",
    "            r = requests.get(url)\n",
    "            with open(zip_filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        \n",
    "        # Extraer el archivo\n",
    "        print(f\"Extrayendo {txt_filename}...\")\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extract(txt_filename)\n",
    "    \n",
    "    # Cargar vectores\n",
    "    print(f\"Cargando vectores GloVe de dimensión {dim}...\")\n",
    "    embeddings = {}\n",
    "    with open(txt_filename, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            valores = line.split()\n",
    "            palabra = valores[0]\n",
    "            vector = np.array([float(val) for val in valores[1:]])\n",
    "            embeddings[palabra] = vector\n",
    "            \n",
    "            # Para mostrar progreso\n",
    "            if i % 100000 == 0:\n",
    "                print(f\"  Procesadas {i} líneas...\")\n",
    "    \n",
    "    print(f\"Cargados {len(embeddings)} vectores GloVe.\")\n",
    "    return embeddings\n",
    "\n",
    "# Cargar vectores GloVe\n",
    "try:\n",
    "    glove_vectors = cargar_glove(dim=100)\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar GloVe: {e}\")\n",
    "    print(\"Creando diccionario vacío como fallback.\")\n",
    "    glove_vectors = {}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ea5aa83-2492-4902-ba19-669cb5a36bd2",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "def encontrar_verbo_mas_frecuente(documento_normalizado):\n",
    "    \"\"\"\n",
    "    Encuentra el verbo más frecuente en un documento normalizado.\n",
    "    \n",
    "    Args:\n",
    "        documento_normalizado (dict): Documento normalizado\n",
    "        \n",
    "    Returns:\n",
    "        str: Verbo más frecuente\n",
    "    \"\"\"\n",
    "    contador_verbos = Counter()\n",
    "    \n",
    "    # Contar todos los verbos\n",
    "    for oracion in documento_normalizado[\"oraciones_etiquetadas\"]:\n",
    "        for palabra, pos in oracion:\n",
    "            if pos == 'VERB':\n",
    "                contador_verbos[palabra.lower()] += 1\n",
    "    \n",
    "    # Obtener el verbo más frecuente\n",
    "    if contador_verbos:\n",
    "        return contador_verbos.most_common(1)[0][0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Encontrar el verbo más frecuente en cada documento\n",
    "verbos_frecuentes = []\n",
    "\n",
    "for i, doc in enumerate(documentos_synsets):\n",
    "    verbo = encontrar_verbo_mas_frecuente(doc)\n",
    "    verbos_frecuentes.append(verbo)\n",
    "    print(f\"Documento {i+1}: Verbo más frecuente -> '{verbo}'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b37c26cf-be34-4f67-9159-8444b95ed3e6",
   "metadata": {},
   "source": [
    "def encontrar_similares_glove(palabra, embeddings, top_n=5):\n",
    "    \"\"\"\n",
    "    Encuentra términos similares a una palabra usando embeddings GloVe y similitud de coseno.\n",
    "    \n",
    "    Args:\n",
    "        palabra (str): Palabra de referencia\n",
    "        embeddings (dict): Diccionario de embeddings GloVe\n",
    "        top_n (int): Número de términos similares a devolver\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tuplas (palabra, similitud)\n",
    "    \"\"\"\n",
    "    # Verificar si la palabra está en el vocabulario\n",
    "    palabra = palabra.lower()\n",
    "    if palabra not in embeddings:\n",
    "        return []\n",
    "    \n",
    "    # Vector de la palabra de referencia\n",
    "    vector = embeddings[palabra]\n",
    "    vector = vector.reshape(1, -1)\n",
    "    \n",
    "\n",
    "    similitudes = []\n",
    "    for otra_palabra, otro_vector in embeddings.items():\n",
    "        if otra_palabra == palabra:\n",
    "            continue\n",
    "        \n",
    "        otro_vector = otro_vector.reshape(1, -1)\n",
    "\n",
    "        sim = cosine_similarity(vector, otro_vector)[0][0]\n",
    "        similitudes.append((otra_palabra, sim))\n",
    "\n",
    "    similitudes.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similitudes[:top_n]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af1fe2ac-d8a5-47b6-b986-501a8753ed5d",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "for i, verbo in enumerate(verbos_frecuentes):\n",
    "    if verbo and verbo in glove_vectors:\n",
    "        similares = encontrar_similares_glove(verbo, glove_vectors)\n",
    "        \n",
    "        if similares:\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            palabras = [palabra for palabra, _ in similares]\n",
    "            similitudes = [similitud for _, similitud in similares]\n",
    "\n",
    "            plt.barh(range(len(palabras)), similitudes, align='center', color='skyblue')\n",
    "            plt.yticks(range(len(palabras)), palabras)\n",
    "\n",
    "            plt.xlabel('Similitud de coseno')\n",
    "            plt.title(f'Documento {i+1}: 5 términos más similares a \"{verbo}\"')\n",
    "            plt.xlim(0, 1)  # Escala de similitud de coseno: 0-1\n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Similitud de documentos con \"embedding\"\n",
    "\n",
    "En esta sección, implementaremos la comparación de similitud entre documentos utilizando el modelo pre-entrenado BERT (Bidirectional Encoder Representations from Transformers). A diferencia del enfoque basado en synsets que usamos anteriormente, BERT captura el significado contextual completo de los documentos.\n",
    "\n",
    "Utilizaremos específicamente el modelo \"bert-base-uncased\", que no hace distinción entre mayúsculas y minúsculas, para obtener embeddings de documentos y calcular su similitud mediante la similitud del coseno."
   ],
   "id": "7a8d1ba2840d97cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    !pip install transformers\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    !pip install torch"
   ],
   "id": "e002f3992da2ca36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "print(\"Cargando modelo BERT 'bert-base-uncased'...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "model = model.to(device)"
   ],
   "id": "34fabc38bd7d267a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def obtener_embedding_bert(texto, max_length=512):\n",
    "    \"\"\"\n",
    "    Obtiene el embedding de un documento usando BERT.\n",
    "\n",
    "    Args:\n",
    "        texto (str): Texto del documento\n",
    "        max_length (int): Longitud máxima de tokens\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Vector de embedding del documento\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", padding=True,\n",
    "                      truncation=True, max_length=max_length)\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "    return embedding\n",
    "\n",
    "embeddings_documentos = []\n",
    "tiempos = []\n",
    "\n",
    "print(\"Calculando embeddings BERT para cada documento...\")\n",
    "for i, doc in enumerate(cuerpo):\n",
    "    inicio = time.time()\n",
    "    print(f\"Procesando documento {i+1}...\")\n",
    "\n",
    "    embedding = obtener_embedding_bert(doc)\n",
    "    embeddings_documentos.append(embedding)\n",
    "\n",
    "    fin = time.time()\n",
    "    tiempo = fin - inicio\n",
    "    tiempos.append(tiempo)\n",
    "    print(f\"  Tiempo: {tiempo:.2f} segundos\")"
   ],
   "id": "f330c2559d32bdb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nSimilitud entre documentos usando embeddings BERT:\")\n",
    "print(f\"{'-'*50}\")\n",
    "\n",
    "num_docs = len(embeddings_documentos)\n",
    "matriz_similitud = np.zeros((num_docs, num_docs))\n",
    "\n",
    "for i in range(num_docs):\n",
    "    for j in range(i, num_docs):\n",
    "\n",
    "        sim = cosine_similarity(embeddings_documentos[i], embeddings_documentos[j])[0][0]\n",
    "        matriz_similitud[i, j] = sim\n",
    "        matriz_similitud[j, i] = sim\n",
    "\n",
    "        if i != j:\n",
    "            print(f\"Documento {i+1} ↔ Documento {j+1}: {sim:.4f}\")"
   ],
   "id": "aa11ff1768b8634",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_similitud = pd.DataFrame(matriz_similitud)\n",
    "df_similitud.index = [f'Doc {i+1}' for i in range(num_docs)]\n",
    "df_similitud.columns = [f'Doc {i+1}' for i in range(num_docs)]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_similitud, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Similitud entre documentos usando BERT')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "doc_base_idx = 0\n",
    "similitudes_bert = []\n",
    "\n",
    "for i in range(num_docs):\n",
    "    if i != doc_base_idx:\n",
    "        similitudes_bert.append((i, matriz_similitud[doc_base_idx, i]))\n",
    "\n",
    "similitudes_bert.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Documento base ({doc_base_idx + 1})\")\n",
    "print(\"\\nRanking de similitud (BERT):\")\n",
    "for i, sim in similitudes_bert:\n",
    "    print(f\"Documento {i+1}: {sim:.4f}\")"
   ],
   "id": "7fcf1c8f977b47d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e4d0e5ac1f3dfd31"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
